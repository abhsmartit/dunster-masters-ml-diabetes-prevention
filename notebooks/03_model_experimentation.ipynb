{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49ad4e7",
   "metadata": {},
   "source": [
    "# Model Experimentation\n",
    "## Masters in AI & ML Project\n",
    "\n",
    "This notebook experiments with different ML models and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.data_processing import DataProcessor\n",
    "from src.model_development import ModelDeveloper\n",
    "from src.model_evaluation import ModelEvaluator\n",
    "from src.utils import save_model, set_plot_style\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "set_plot_style()\n",
    "print('‚úì Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ae162",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset using DataProcessor\n",
    "# For demonstration, we'll create a sample dataset\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
    "                          n_redundant=5, n_classes=2, random_state=42)\n",
    "df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(20)])\n",
    "df['Target'] = y\n",
    "\n",
    "# Save to CSV for DataProcessor\n",
    "temp_path = '../data/raw/sample_data.csv'\n",
    "df.to_csv(temp_path, index=False)\n",
    "\n",
    "# Use DataProcessor\n",
    "processor = DataProcessor(temp_path)\n",
    "processor.load_data()\n",
    "processor.explore_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X_train, X_test, y_train, y_test = processor.prepare_data(\n",
    "    target_column='Target',\n",
    "    test_size=0.2,\n",
    "    scale=True\n",
    ")\n",
    "\n",
    "print(f'\\n‚úì Data prepared:')\n",
    "print(f'   Training set: {X_train.shape}')\n",
    "print(f'   Test set: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede062f",
   "metadata": {},
   "source": [
    "## 2. Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda67c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ModelDeveloper\n",
    "developer = ModelDeveloper(task_type='classification', random_state=42)\n",
    "\n",
    "# Train all default models\n",
    "trained_models = developer.train_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7378a",
   "metadata": {},
   "source": [
    "## 3. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b6e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ModelEvaluator\n",
    "evaluator = ModelEvaluator(task_type='classification')\n",
    "\n",
    "# Evaluate all models\n",
    "comparison_df = evaluator.evaluate_models(trained_models, X_test, y_test)\n",
    "\n",
    "# Display results\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25300f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig = evaluator.plot_model_comparison(comparison_df, metric='Accuracy')\n",
    "plt.show()\n",
    "\n",
    "fig = evaluator.plot_model_comparison(comparison_df, metric='F1-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028339d6",
   "metadata": {},
   "source": [
    "## 4. Detailed Analysis of Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95177de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 3 models\n",
    "top_models = comparison_df.head(3)['Model'].tolist()\n",
    "print(f'\\nüèÜ Top 3 Models: {top_models}')\n",
    "\n",
    "# Analyze each top model\n",
    "for model_name in top_models:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Analyzing: {model_name}')\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = trained_models[model_name]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig = evaluator.plot_confusion_matrix(y_test, y_pred, model_name)\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve (if probabilities available)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        fig = evaluator.plot_roc_curve(y_test, y_pred_proba, model_name)\n",
    "        plt.show()\n",
    "    \n",
    "    # Feature Importance (if available)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        fig = evaluator.plot_feature_importance(\n",
    "            model, X_train.columns.tolist(), model_name, top_n=15\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acfdaa0",
   "metadata": {},
   "source": [
    "## 5. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f996c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on top models\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "cv_results = {}\n",
    "for model_name in top_models:\n",
    "    model = trained_models[model_name]\n",
    "    cv_result = developer.cross_validate_model(model, X_full, y_full, cv=5)\n",
    "    cv_results[model_name] = cv_result\n",
    "    print(f'{model_name}: {cv_result[\"mean\"]:.4f} ¬± {cv_result[\"std\"]:.4f}')\n",
    "\n",
    "# Visualize CV results\n",
    "cv_means = [cv_results[name]['mean'] for name in top_models]\n",
    "cv_stds = [cv_results[name]['std'] for name in top_models]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(top_models, cv_means, xerr=cv_stds, capsize=5)\n",
    "plt.xlabel('Cross-Validation Accuracy')\n",
    "plt.title('5-Fold Cross-Validation Results')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50c0c64",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a251f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the best performing model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "print(f'\\nüîß Tuning hyperparameters for: {best_model_name}')\n",
    "\n",
    "# Get parameter grid (example for Random Forest)\n",
    "if 'Random Forest' in best_model_name:\n",
    "    from src.model_development import CLASSIFICATION_PARAM_GRIDS\n",
    "    param_grid = CLASSIFICATION_PARAM_GRIDS['Random Forest']\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    base_model = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Perform tuning (this may take a while)\n",
    "    tuned_model = developer.hyperparameter_tuning(\n",
    "        base_model, param_grid, X_train, y_train, cv=3\n",
    "    )\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_metrics = evaluator.evaluate_model(\n",
    "        tuned_model, X_test, y_test, f'{best_model_name} (Tuned)'\n",
    "    )\n",
    "    \n",
    "    print(f'\\nüìä Tuned Model Performance:')\n",
    "    print(f'   Accuracy: {tuned_metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'   F1-Score: {tuned_metrics[\"f1_score\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72840a",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model = trained_models[best_model_name]\n",
    "model_path = f'../models/best_model_{best_model_name.replace(\" \", \"_\")}.joblib'\n",
    "\n",
    "save_model(best_model, model_path)\n",
    "print(f'\\n‚úì Best model saved: {best_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261f699",
   "metadata": {},
   "source": [
    "## 8. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f634698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed report for best model\n",
    "report = evaluator.generate_evaluation_report(best_model_name)\n",
    "print(report)\n",
    "\n",
    "# Save report to file\n",
    "report_path = f'../results/model_report_{best_model_name.replace(\" \", \"_\")}.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f'\\n‚úì Report saved to: {report_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db165d",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('MODEL EXPERIMENTATION SUMMARY')\n",
    "print('='*60)\n",
    "print(f'\\nüèÜ Best Model: {best_model_name}')\n",
    "print(f'üìä Test Accuracy: {comparison_df.iloc[0][\"Accuracy\"]:.4f}')\n",
    "print(f'üìà F1-Score: {comparison_df.iloc[0][\"F1-Score\"]:.4f}')\n",
    "print(f'\\n‚úì Total models trained: {len(trained_models)}')\n",
    "print(f'‚úì Best model saved to: {model_path}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('RECOMMENDATIONS')\n",
    "print('='*60)\n",
    "print('1. Further hyperparameter tuning for improved performance')\n",
    "print('2. Ensemble methods combining top models')\n",
    "print('3. Feature engineering based on domain knowledge')\n",
    "print('4. Collect more data if model performance is insufficient')\n",
    "print('5. Consider model interpretability for deployment')\n",
    "print('\\n‚úì Model Experimentation Complete!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
